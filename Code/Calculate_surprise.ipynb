{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "from scipy.stats import pearsonr\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import copy\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "stimuli_location = \"Stimuli/Text\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Surprisal values.\n",
    "\n",
    "Read in the word vectors, divide them into sentences based on the sentence boundries provided, and read in the word vectors as a gensim model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: string dictating the location of the RunX.mat files\n",
    "# output: a three dimensional list with the words of shape: #runs x #sentences x #words\n",
    "\n",
    "def read_mat(location):\n",
    "    \n",
    "    all_runs = [] \n",
    "\n",
    "    for i in range(1,21): # for each run\n",
    "        run = []\n",
    "        loc = stimuli_location + \"/Run\" + str(i) + \".mat\"    \n",
    "        words = sio.loadmat(loc)\n",
    "        j = 0\n",
    "        boundrys = words['sentence_boundaries'][0]\n",
    "\n",
    "        if i == 4:                    # fix wrong last sentence boundry in run 4 (source file mistake). \n",
    "            boundrys[-1] = 181\n",
    "\n",
    "        for boundry in boundrys:  # for each sentence\n",
    "            sentence = []\n",
    "            for word in words['wordVec'][j:]:  # for each word \n",
    "                if words['onset_time'][j] < boundry:    # check if word is in sentence, if so add, else go to next sentence.\n",
    "                    sentence.append(word[0][0].lower()) \n",
    "                    j+=1\n",
    "                else:\n",
    "                    break\n",
    "            run.append(sentence)\n",
    "        all_runs.append(run)\n",
    "    \n",
    "    return all_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the pearsons correlations.\n",
    "\n",
    "Here the pearson's correlations are calculated for each word with the previous seen words in the sentence.\n",
    "When there are no previous words in the sentence we look at the full previous sentence instead.\n",
    "But what if it's the first word of a run? No documentation on this in the broderick paper. \n",
    "\n",
    "We set the similarity to 1. which later on gets changed to a dissimilarity of 0 and hence gets ignored by the regression model.\n",
    "Another issue is if the previous sentence is an empty sentence (happens sometimes due to us only looking at content words), what do you do then? no documentation again. We look back until a non empty sentence is found but this might not be the right solution. Might be better to just ignore these words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: the word embedding and the embeddings of the sentence it occurs in. (up to the word)\n",
    "# output: the correlation between the word embedding and the average of the sentence embeddings \n",
    "\n",
    "def calc_cor(word, sentence, model):\n",
    "    avg = np.mean(np.stack([model[word2] for word2 in sentence]),0)  \n",
    "    cor, _ = pearsonr(model[word],avg)\n",
    "    return cor\n",
    "\n",
    "\n",
    "# calculate the similarity vectors,\n",
    "# note that you run into problems when you enctounter sentences of size 0, \n",
    "# we simply look back multiple times to fix this\n",
    "\n",
    "# input: 3d words list generated by read_mat\n",
    "# output: same format as input but now contains similarity values instead of the words themself \n",
    "\n",
    "def broderick_values(all_runs_words):\n",
    "\n",
    "    # read in the relevant word vectors.\n",
    "    model = KeyedVectors.load_word2vec_format(\"limited_vectors.txt\", binary=False, encoding=\"utf8\")\n",
    "\n",
    "\n",
    "    all_runs_sim = []\n",
    "    for run in all_runs_words: # for all runs\n",
    "        run_sim = []\n",
    "        for i, sentence in enumerate(run): # for all sentences\n",
    "            sentence_sim = []\n",
    "            for j, word in enumerate(sentence): # for all words, \n",
    "\n",
    "                # check if its the first word of the sentence or not.\n",
    "                if j != 0: \n",
    "\n",
    "                    # if not then avarage the vectors of the previous seen words in this sentence and calculate the correlation.\n",
    "                    sentence_sim.append(calc_cor(word, sentence[:j], model))\n",
    "                else:\n",
    "\n",
    "                    # if it was the first word in sentence but not the first sentence.\n",
    "                    if i != 0:\n",
    "\n",
    "                        # if the previous sentence is not empty.\n",
    "                        if run[i-1] !=[]: \n",
    "\n",
    "                            # if not then avarage the vectors of the previous sentence and calculate the correlation.\n",
    "                            sentence_sim.append(calc_cor(word, run[i-1], model))\n",
    "                        else:\n",
    "\n",
    "                            # if that previous previous is also not empty\n",
    "                            if run[i-2] !=[]:\n",
    "\n",
    "                                # if not then avarage the vectors of the previous sentence and calculate the correlation.\n",
    "                                sentence_sim.append(calc_cor(word, run[i-2], model))\n",
    "                            else:\n",
    "                                # if not then avarage the vectors of the previous sentence and calculate the correlation.\n",
    "                                sentence_sim.append(calc_cor(word, run[i-3], model))\n",
    "                    else:\n",
    "\n",
    "                        # if it the first word in the first sentence then append a 1 as simiilarity. which becomes a 0 disimilarity later\n",
    "                        sentence_sim.append(1)\n",
    "            run_sim.append(sentence_sim)\n",
    "        all_runs_sim.append(run_sim)\n",
    "    \n",
    "    return all_runs_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BERT masked language task instead.\n",
    "\n",
    "The BERT langauge model is used to predict the word in its sentence. Its prediction score is used as a measure of similarity. The lower the score the higher the dissimilarity. The get normalized at the end.\n",
    "\n",
    "This part is somewhat unfinshed. While we have initial results, there are still a few problems in the implementation that need to be addressed. they are:\n",
    " - The model also looks ahead in the sentence.\n",
    " - some words get split into subwords by the tokenizer, they are then ignored in this implementation, better solution needs to be found.\n",
    " - small sentences often don't perform too well as there is not enough information to base a good prediction on.\n",
    "     - possible solution to this problem is to give the last n sentences as input instead of only the current one, this way you also solve the first word of sentence problem when its no longer allowed to look into the future of the sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create for each word a copy of the sentence its in with itself masked as the prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def create_masked_input(tokenizer,inputs,context_size, future_look=False):\n",
    "    \n",
    "    #creating a deep copy that we can adjust as needed \n",
    "    all_runs2 = copy.deepcopy(inputs)\n",
    "    \n",
    "    # these empty sentences need to be removed because they do not occur in the real sentence\n",
    "    # there are other empty sentences, but those can be explained by being short sentence with no target words.\n",
    "    # not sure why they are contained in the proveded dataset, a bug? \n",
    "    del all_runs2[3][16]\n",
    "    del all_runs2[3][36]\n",
    "    del all_runs2[3][38]\n",
    "    del all_runs2[4][15]\n",
    "    \n",
    "    \n",
    "    #loading in the txt file that contains the full sentences.\n",
    "    with open('full_text.txt', 'r', encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    \n",
    "    # initializations\n",
    "    found = 0\n",
    "    skip = 0\n",
    "    sen = 0\n",
    "    not_founds = []\n",
    "    all_masks = []\n",
    "    all_ids = []\n",
    "    \n",
    "    # Que that contains \"context_size\" number of previous sentences\n",
    "    # these are the sentences passed along to bert for predictions along with a portion of the curernt sentence\n",
    "    que = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for l, run in enumerate(all_runs2):\n",
    "        run_ids = []\n",
    "        masked_run = []\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i, line in enumerate(run):\n",
    "            line_ids = []\n",
    "            masked_line = []\n",
    "            \n",
    "            sentence = lines[sen]\n",
    "            t_sentence = tokenizer.tokenize(sentence)\n",
    "            flatten_que = [wword for ssentence in que for wword in ssentence]\n",
    "            \n",
    "            #print(\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\")\n",
    "            #print(\"full sentence:\" ,t_sentence, \"\\n\")\n",
    "            #print(\"words:\", line, \"\\n\")\n",
    "            \n",
    "            seen_upto = 0\n",
    "            for j, word in enumerate(line):\n",
    "\n",
    "                if word in t_sentence:\n",
    "                    \n",
    "                    # find the index of the word in the sentence, and transform that word in the mask.\n",
    "                    # look only in the part of the sentence you havent seen yet for the previous word\n",
    "                    # in order to handle duplicate words in the sentence. (done using \"seen_upto\")\n",
    "                    ind = seen_upto + t_sentence[seen_upto:].index(word)\n",
    "                    mask = t_sentence[:ind]\n",
    "                    line_ids.append(len(flatten_que) + ind)\n",
    "                    mask = mask + ['[MASK]']\n",
    "                    \n",
    "                    if future_look :\n",
    "                        mask = mask + t_sentence[ind+1:]\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    #print(que + mask)\n",
    "                    #print(ASDfasdf)\n",
    "                    \n",
    "                    # add to output and update variables\n",
    "                    masked_line.append(flatten_que + mask)\n",
    "                    found += 1\n",
    "                    seen_upto = ind\n",
    "                    \n",
    "                    #print(flatten_que + mask)\n",
    "                else:\n",
    "                    #print(word)\n",
    "                    \n",
    "                    # if word not found: append as None in output (to catch in next steps)\n",
    "                    line_ids.append(None)\n",
    "                    not_founds.append(word)\n",
    "                    skip +=1\n",
    "\n",
    "                    \n",
    "                #que management\n",
    "            que.append(t_sentence)\n",
    "            if len(que) > context_size:\n",
    "                #print(\"asdfasdfasdfasdfasdfasdfasdfasdfasdfasdfasdfasdfasdfasdfasdfasdfasdfasdfasdfasd\")\n",
    "                que.pop(0)\n",
    "                \n",
    "            # add to higher lists\n",
    "            masked_run.append(masked_line)\n",
    "            run_ids.append(line_ids)\n",
    "            sen+=1\n",
    "        \n",
    "        # add to higher lists\n",
    "        all_masks.append(masked_run)\n",
    "        all_ids.append(run_ids)\n",
    "            \n",
    "        \n",
    "    return(all_masks,all_ids,all_runs2)\n",
    "\n",
    "    \n",
    "#test1, test2 = create_masked_input(all_runs,1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def bert_prediction(tokenizer, masked_sentence_runs, all_runs_ids, words, printt=False):\n",
    "    # load the pretrained model.\n",
    "\n",
    "    model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "    model.eval()\n",
    "\n",
    "    bert_scores_runs = []\n",
    "\n",
    "    for l,run in enumerate(tqdm(all_runs_ids)): # for all runs\n",
    "        bert_scores_lines = []      \n",
    "        \n",
    "        for i,line in enumerate(run):  # for all sentences.\n",
    "            ii = 0\n",
    "            bert_scores_words = []\n",
    "            \n",
    "            for j, idx in enumerate(line):\n",
    "                \n",
    "                if idx != None:\n",
    "                    inp = ['[CLS]'] + masked_sentence_runs[l][i][ii]\n",
    "                    ii += 1\n",
    "                    indexed_tokens = tokenizer.convert_tokens_to_ids(inp)\n",
    "                    segments_ids = [0]* len(indexed_tokens)\n",
    "\n",
    "    #                 tokens_tensor = torch.tensor([indexed_tokens]).cuda()\n",
    "    #                 segments_tensors = torch.tensor([segments_ids]).cuda()\n",
    "\n",
    "                    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "                    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "                    # ask bert for predictions for the masked sentence.\n",
    "                    with torch.no_grad():\n",
    "                        predictions = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "                    orig_word = words[l][i][j]\n",
    "                    orig_id = tokenizer.convert_tokens_to_ids([orig_word])\n",
    "\n",
    "                    # get the score of this word.\n",
    "                    score = predictions[0,idx+1][orig_id]\n",
    "                    bert_scores_words.append(score.item())\n",
    "\n",
    "                    # some prints to clearify whats happening.\n",
    "                    predicted_index = predictions[0, idx+1].topk(5)[1].cpu().numpy()\n",
    "                    predicted_token = tokenizer.convert_ids_to_tokens(predicted_index)\n",
    "\n",
    "\n",
    "\n",
    "                    if printt:\n",
    "                        print('>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<< run: ', l,' line: ', i)\n",
    "                        print(\" \")\n",
    "                        \n",
    "                        print(\"sentence:\", inp)\n",
    "                        print(\"target word:\",orig_word)\n",
    "                        print(\"token id:\",orig_id)\n",
    "                        print(\"score given:\",score.item())\n",
    "                        print(\"top 5 predictions\",predicted_token)\n",
    "                        print(\"scores of top 5:\",predictions[0, idx+1].topk(5)[0].cpu().numpy())\n",
    "                        print(\" \") \n",
    "\n",
    "\n",
    "\n",
    "                else:\n",
    "                    bert_scores_words.append(None)\n",
    "            bert_scores_lines.append(bert_scores_words)\n",
    "\n",
    "        bert_scores_runs.append(bert_scores_lines)\n",
    "        \n",
    "    return bert_scores_runs\n",
    "\n",
    "#bert_prediction(test1, test2,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those scores are ofcourse not normalized, they seem to be somewhere around -3 to 10 but need to be normalized first.\n",
    "This is done by this calculation: (value - minimum) / (maximum-minimum)\n",
    "The results are scores normalized between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_bert(bert_scores_runs):\n",
    "\n",
    "    total = 0\n",
    "    count = 0\n",
    "    flat_lst = []\n",
    "\n",
    "    # create a flat list of values to extract max /min etc from, also calc total here.\n",
    "    for run in bert_scores_runs:\n",
    "        for line in run:\n",
    "            for word in line:\n",
    "                if word != None:\n",
    "                    flat_lst.append(word)\n",
    "                    total+= word\n",
    "                    count+= 1 \n",
    "\n",
    "    # some prints and precalculating some values\n",
    "    print('average', total/count)\n",
    "    print('sd', np.std(flat_lst))\n",
    "    u = np.mean(flat_lst)\n",
    "    s = np.std(flat_lst)\n",
    "    mi = np.min(flat_lst)\n",
    "    ma = np.max(flat_lst)\n",
    "    norm = ma-mi\n",
    "\n",
    "    # applying the normalizations.\n",
    "    bert_normalized_runs = []\n",
    "    for run in bert_scores_runs:\n",
    "        bert_normalized_lines = []\n",
    "        for line in run:\n",
    "            bert_normalized_words = []\n",
    "            for word in line:\n",
    "                if word != None:\n",
    "                    bert_normalized_words.append((word - mi)/ norm)\n",
    "                else: \n",
    "                    bert_normalized_words.append(0) #### should be 1?\n",
    "            bert_normalized_lines.append(bert_normalized_words)\n",
    "        bert_normalized_runs.append(bert_normalized_lines)\n",
    "\n",
    "    \n",
    "    return bert_normalized_runs\n",
    "    \n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input: method either \"broderick\" or \"bert\"\n",
    "\n",
    "def create_disimilarity_values(method, context_size = 1, future_look = False, printt = False, save=True):\n",
    "    all_runs_words = read_mat(\"Stimuli/Text\")\n",
    "    \n",
    "    if method == \"pearson\":\n",
    "        result = broderick_values(all_runs_words)\n",
    "        \n",
    "    if method == \"bert\":\n",
    "        # tokenizer used\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        masked_sentence_runs, all_runs_ids, all_runs = create_masked_input(tokenizer, all_runs_words, context_size, future_look)\n",
    "        bert_scores_runs = bert_prediction(tokenizer, masked_sentence_runs, all_runs_ids, all_runs, printt)\n",
    "        result = normalise_bert(bert_scores_runs)\n",
    "        \n",
    "    if method == \"static\":\n",
    "        result = [[[1 for word in sen] for sen in run] for run in all_runs_words]\n",
    "        \n",
    "    if save:\n",
    "        if method == \"bert\":\n",
    "            np.save('vectors/' + method +  '_' + str(context_size) +  '_' + str(future_look) + '.npy', np.array(result))\n",
    "        else:\n",
    "            np.save('vectors/' + method + '.npy', np.array(result))\n",
    "        \n",
    "    return result\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the commands to create the main variants of the suprisal values discussed in the thesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = create_disimilarity_values(\"pearson\") # pearson.npy\n",
    "# res = create_disimilarity_values(\"static\") # static.npy\n",
    "\n",
    "#res = create_disimilarity_values(\"bert\", 0, True) # bert_0_True.npy.npy\n",
    "# res = create_disimilarity_values(\"bert\", 1, True) # bert_1_True.npy.npy\n",
    "# res = create_disimilarity_values(\"bert\", 2, True) # bert_2_True.npy.npy\n",
    "# res = create_disimilarity_values(\"bert\", 3, True) # bert_3_True.npy.npy\n",
    "# res = create_disimilarity_values(\"bert\", 4, True) # bert_4_True.npy.npy\n",
    "\n",
    "# res = create_disimilarity_values(\"bert\", 0, False) # bert_0_False.npy.npy\n",
    "# res = create_disimilarity_values(\"bert\", 1, False) # bert_1_False.npy.npy\n",
    "# res = create_disimilarity_values(\"bert\", 2, False) # bert_2_False.npy.npy\n",
    "# res = create_disimilarity_values(\"bert\", 3, False) # bert_3_False.npy.npy\n",
    "res = create_disimilarity_values(\"bert\", 6, False) # bert_4_False.npy.npy\n",
    "\n",
    "# test = create_disimilarity_values(\"static\", 5, False, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122,
   "position": {
    "height": "40px",
    "left": "741px",
    "right": "20px",
    "top": "113px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
