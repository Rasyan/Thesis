{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import mne\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from mne.decoding import ReceptiveField\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import random\n",
    "\n",
    "# location of files\n",
    "stimuli_location = \"Stimuli/Text\"\n",
    "eeg_location = \"EEG/Subject\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Suprisal vector.\n",
    "\n",
    "Now that the similarity measures for each word have been calculated, we need to create the actual disimilarity vector. Basicly the similarity values are put into the right place on the vector that corrosponds with the onset of the word. They are also subtracted from 1 to create dismilarity values from similarity values ( both values are between 0-1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep things relatively clean looking, comment out for more information.\n",
    "\n",
    "mne.set_log_level('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some general variables.\n",
    "\n",
    "# what subjects to process. (i only included subject 19 data here)\n",
    "subs =[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]\n",
    "# subs =[19]\n",
    "\n",
    "# the similarity vectors to use (that we calculated above)\n",
    "# pearson.npy for broderic dissimilarity values\n",
    "# bert-scores.npy for bert dissimilarity values\n",
    "#similarity_location = 'vectors/pearson.npy'\n",
    "vector_loc = 'vectors/'\n",
    "#vector = 'static'\n",
    "\n",
    "# similarity_location = 'vectors/bert_2sen_onedir2.npy'\n",
    "\n",
    "# frequency of the EEG data\n",
    "sfreq = 128\n",
    "\n",
    "# number of channels\n",
    "n_channels = 128\n",
    "\n",
    "# Define the delays that we will use in the receptive field (0 = event, so this means we take into account upto 0.6seconds after event)\n",
    "tmin, tmax = 0, .6\n",
    "\n",
    "# the mean similarity for the broderick method, everything is normalised to this.\n",
    "norm = 0.33820258789060476\n",
    "\n",
    "results_folder = 'results/'\n",
    "second_folder = 'randoms/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def saveFunc(loc, run_string , scores_subs, coefs_subs, error_subs, length):\n",
    "    \n",
    "    if length > 1:\n",
    "        np.save( results_folder +  'coefs/' + run_string + '_' + length + '.npy', np.array(coefs_subs))\n",
    "        np.save( results_folder + 'scores/' + run_string + '_' + length + '.npy', np.array(scores_subs))\n",
    "        np.save( results_folder + 'errors/' + run_string + '_' + length + '.npy', np.array(error_subs))\n",
    "       \n",
    "    else: \n",
    "        np.save( results_folder +  'coefs/' + run_string + '.npy', np.array(coefs_subs))\n",
    "        np.save( results_folder + 'scores/' + run_string + '.npy', np.array(scores_subs))\n",
    "        np.save( results_folder + 'errors/' + run_string + '.npy', np.array(error_subs))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sets the similarity values (1- the value for disimilarity) of each words at the onset of the word, thus creating a disimilarity vector. loads the eeg data to create a dismilarity vector of the same size. Takes the opportunity to save all the eeg data into lists of lists to prevent reloading it later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_eeg_and_disimilarlity_vector(sim, method):\n",
    "\n",
    "    # flatten the similarity values (from 3D list to 1D list)\n",
    "    flat_sim = [word for sentence in [sentence for run in sim for sentence in run] for word in sentence]\n",
    "    \n",
    "    # normalize to the values of the broderick similarity values.\n",
    "    normalise = norm/np.mean(flat_sim)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # \n",
    "    raw_subs = []\n",
    "    ms_subs = []\n",
    "    dis_vector_subs = []\n",
    "\n",
    "    for sub in tqdm(subs):  # for each subject\n",
    "        raw = []\n",
    "        ms = []\n",
    "        dis_vector = []\n",
    "        #print('Load eeg & dissimilarity values: subject: ', sub, \"/\", len(subs), end = \"\\r\", flush=True)\n",
    "\n",
    "        for i in range(1,21):  # for each run\n",
    "\n",
    "            # load data\n",
    "            data = sio.loadmat(eeg_location + str(sub) + \"/Subject\" + str(sub) + \"_Run\" + str(i) +  \".mat\")\n",
    "            raw_i = data['eegData'].T\n",
    "            ms_i = data['mastoids'].T\n",
    "            loc = stimuli_location + \"/Run\" + str(i) + \".mat\"\n",
    "            words = sio.loadmat(loc)\n",
    "\n",
    "            # create list of word similaritys in order and normalise.\n",
    "            norm_flat_sim = [simz * normalise for sentence in sim[i-1] for simz in sentence]\n",
    "\n",
    "\n",
    "            if len(norm_flat_sim)!= len(words['onset_time']):\n",
    "                print(sub,i, len(norm_flat_sim), len(words['onset_time']))\n",
    "\n",
    "            # create empty vector of same size as the eeg.\n",
    "            dis_vector_i = np.zeros((1,raw_i.shape[1]))\n",
    "            \n",
    "            if method == 'random_loc': \n",
    "                random_locations = random.sample(range(0,raw_i.shape[1]), len(norm_flat_sim))\n",
    "            \n",
    "            #print(asdfasdfasdfasdf)\n",
    "            # for each word, find its onset, and then place it there in the empty vector.\n",
    "            for j in range(len(norm_flat_sim)):\n",
    "                \n",
    "                \n",
    "                if method == 'random_loc':\n",
    "                    on = random_locations[j]\n",
    "                else: \n",
    "                    on = int(np.floor(words['onset_time'][j] * sfreq))\n",
    "                    \n",
    "                dis_vector_i[0][on] = (1-norm_flat_sim[j]) \n",
    "            \n",
    "\n",
    "            \n",
    "                # This is also where the random vectors and such are created. \n",
    "                # comment out to do something else then the normal method\n",
    "\n",
    "\n",
    "                # dis_vector_i[0][on] = 0.33820258789060476                     # one at onset\n",
    "                # dis_vector_i[0][on] = random.uniform(0, 1)  # random value on onset\n",
    "\n",
    "    #         for i in range(1,len(dis_vector_i[0])):          # value at specific or all times\n",
    "    #             dis_vector_i[0][i] = 1\n",
    "\n",
    "\n",
    "            raw.append(raw_i)     \n",
    "            dis_vector.append(dis_vector_i)\n",
    "            ms.append(ms_i)\n",
    "\n",
    "        raw_subs.append(raw)\n",
    "        ms_subs.append(ms)\n",
    "        dis_vector_subs.append(dis_vector)\n",
    "        \n",
    "    print('Load eeg & dissimilarity values: Done')\n",
    "    return(dis_vector_subs,ms_subs,raw_subs)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "For each run of each subject, load into mne and preprocess. \n",
    "preprocessing consists of applying a bandpass filter(1 to 8hz) and rerefrencing to the average of the two mastoids. \n",
    "\n",
    "The band pass filter filters out all frequencys outside of the range specified.\n",
    "referencing is done by averaging the two mastoids and subtracting it from each channel. this way head movements and such can be removed from the eeg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_eeg(dis_vector, raw_mastoids, raw_eeg, var_limit): \n",
    "    \n",
    "    fixes = 0\n",
    "    fixes2 = 0\n",
    "    subject_data = []\n",
    "    for sub in tqdm(subs):\n",
    "        #print('Process eeg: subject: ', sub, \"/\", len(subs), end = \"\\r\", flush=True)\n",
    "        raws = np.empty((20,), dtype=object)\n",
    "        ms = np.empty((20,), dtype=object)\n",
    "        dv = np.empty((20,), dtype=object)\n",
    "        filterd = np.empty((20,), dtype=object)\n",
    "        data = {}\n",
    "        for i in range(1,21):\n",
    "\n",
    "            # load data of the prev cell\n",
    "            raws[i-1] = raw_eeg[subs.index(sub)][i-1].T\n",
    "            ms[i-1] = raw_mastoids[subs.index(sub)][i-1].T\n",
    "            dv[i-1] = dis_vector[subs.index(sub)][i-1].T\n",
    "\n",
    "\n",
    "            # add mastoids as the last two channels.\n",
    "            raw = np.concatenate((raw_eeg[subs.index(sub)][i-1],raw_mastoids[subs.index(sub)][i-1]))\n",
    "            \n",
    "            \n",
    "            montage = mne.channels.make_standard_montage('biosemi128');\n",
    "            montage.ch_names = montage.ch_names + ['M1', 'M2']\n",
    "            montage.dig = montage.dig + [montage.dig[4],montage.dig[5]]\n",
    "            \n",
    "\n",
    "            info = mne.create_info(montage.ch_names[:130], sfreq, 'eeg', montage=montage)\n",
    "            raw = mne.io.RawArray(raw, info)\n",
    "            raw.add_channels\n",
    "\n",
    "\n",
    "            # remove the standerd reference mne sets upon it.\n",
    "            raw, _ = mne.set_eeg_reference(raw, [])\n",
    "\n",
    "            # reference instead using the average of the last two channels (mastoids)\n",
    "            raw, _ = mne.set_eeg_reference(raw, montage.ch_names[128:130])\n",
    "\n",
    "            # bandpass filter the data without the mastoids\n",
    "            #raw.info['bad'] = montage.ch_names[128:130]\n",
    "            raw = raw.drop_channels(montage.ch_names[128:130])\n",
    "            raw = raw.filter(1,8)\n",
    "            \n",
    "            test = raw.get_data()\n",
    "        \n",
    "            var = [np.var(channel[1280:-1280]) for channel in test]\n",
    "            raw.info['bads'] = np.array(montage.ch_names[:128])[np.array(var[:128])>var_limit]\n",
    "            fixes2 += raw.info['bads'].size\n",
    "            if raw.info['bads'].size > 0: fixes +=1\n",
    "            raw.interpolate_bads()\n",
    "\n",
    "            \n",
    "            #print(raw.get_data.size())\n",
    "            filt = raw.get_data()\n",
    "            filterd[i-1] = filt[:128].T   \n",
    "\n",
    "\n",
    "        data['raw'] = raws\n",
    "        data['ms'] = ms\n",
    "        data['dv'] = dv\n",
    "        data['filterd'] = filterd\n",
    "        subject_data.append(data)\n",
    "    \n",
    "    print('Process eeg: Done')\n",
    "    print(\"Total fixes: \", fixes)\n",
    "    print(\"Total fixes: \", fixes2)\n",
    "    return(subject_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning\n",
    "\n",
    "learning is done using 5 folds regression, its resulting coeffiecients (weights) are the actual outputs we are looking for. the scores are the prediction accuracy per channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TRF(subject_data, start=0, end=1, save=None, shuffle=False, second_folder = 'randoms/'):\n",
    "    \n",
    "    for run in tqdm(range(start,end)):\n",
    "    \n",
    "        #print('run', run)\n",
    "        scores_subs=[]\n",
    "        coefs_subs=[]\n",
    "        error_subs=[]\n",
    "        for sub in tqdm(subs):\n",
    "\n",
    "            # run value as random seed. makes reproduction possible if we need extra info later on.\n",
    "            random.seed(run)\n",
    "            data = subject_data[subs.index(sub)]\n",
    "            dv = np.vstack(data['dv'])\n",
    "\n",
    "            if shuffle:\n",
    "                # create a list of all disimilarity values.\n",
    "                ddv = np.array([d for d in dv if d>0])\n",
    "\n",
    "                # shuffle this list.\n",
    "                random.shuffle(ddv)\n",
    "\n",
    "                # for each non 0 in real dissimilarity vector\n",
    "                i = 0\n",
    "                for j in range(len(dv)):\n",
    "                    if dv[j] > 0:\n",
    "\n",
    "                        # add the i element of the shuffled list of dissimilarity values.\n",
    "                        dv[j] = ddv[i]\n",
    "                        i+=1\n",
    "\n",
    "            Y = np.vstack(data['filterd']) \n",
    "\n",
    "            # normalisation method. basicly just subtracts the average and devides by the standerd deviations\n",
    "            scaler = StandardScaler()           # initalise scaler\n",
    "            scaler.fit(Y)                       # finds the average and sd\n",
    "            Y = scaler.transform(Y)      # scales each value.\n",
    "\n",
    "\n",
    "            #Initialize the model\n",
    "            rf = ReceptiveField(tmin, tmax, sfreq, feature_names=['envelope'],\n",
    "                                estimator=1., scoring='corrcoef')\n",
    "\n",
    "            # calculate how many items there are between the delays\n",
    "            n_delays = int((tmax - tmin) * sfreq) + 2\n",
    "\n",
    "            # setup 5fold CV\n",
    "            n_splits = 5\n",
    "            cv = KFold(n_splits)\n",
    "\n",
    "\n",
    "            DV = np.array([dv[i-78:i,] for i in range(78,dv.shape[0])])\n",
    "\n",
    "\n",
    "            # Simple linear regression for each time step using 5fold CV\n",
    "            coefs = np.zeros((n_splits, n_channels, n_delays))\n",
    "            scores = np.zeros((n_splits, n_channels))\n",
    "\n",
    "\n",
    "            err= []\n",
    "            # Iterate through splits, fit the model, and predict/test on held-out data\n",
    "            for ii, (train, test) in enumerate(cv.split(dv)):  # split the data 4:1 (train:test) for each different way.\n",
    "                #print('split %s / %s' % (ii + 1, n_splits))\n",
    "\n",
    "                rf.fit(dv[train], Y[train])\n",
    "\n",
    "\n",
    "                scores[ii] = rf.score(dv[test], Y[test])\n",
    "                # coef_ is shape (n_outputs, n_features, n_delays). we only have 1 feature\n",
    "                coefs[ii] = rf.coef_[:, 0, :]\n",
    "\n",
    "                # calculate errors\n",
    "\n",
    "                W = np.flip(coefs[ii],1) # define Weights as the flipped coefs\n",
    "                #print(test)\n",
    "\n",
    "                test=test[test>=78] # remove the first 78 values, as we dont have full data for them\n",
    "                test=test -78 \n",
    "                #print(test)\n",
    "                DV2 = DV[test] # select only the dissimilarity values for the test set\n",
    "                real = Y[test]\n",
    "\n",
    "\n",
    "                DVs = np.sum(DV2,1) \n",
    "                mask = [s for s in range(0,len(DV2)) if DVs[s]!=0]\n",
    "                DV2 = DV2[mask] # remove all test cases with 0 dv values\n",
    "                real = real[mask]\n",
    "\n",
    "\n",
    "                pred = np.array([W @ s for s in DV2]).squeeze() # calculate predicted value.\n",
    "\n",
    "                m = np.mean(real,0)\n",
    "                SStot = np.sum((real-m)**2,0)\n",
    "                SSreg = np.sum((pred-m)**2,0)\n",
    "                SSres = np.sum((real-pred)**2,0)\n",
    "                r2 = 1-(SSres/SStot)\n",
    "                errors = np.array([SStot,SSreg,SSres,r2])\n",
    "\n",
    "                err.append(errors)\n",
    "\n",
    "\n",
    "\n",
    "            times = rf.delays_ / float(rf.sfreq)\n",
    "\n",
    "\n",
    "            scores_subs.append(scores)\n",
    "            coefs_subs.append(coefs)\n",
    "            error_subs.append(err)\n",
    "        \n",
    "        if shuffle==True and save and ((end-start)>1):\n",
    "            # save results\n",
    "            saveFunc(loc, run_string , scores_subs, coefs_subs, error_subs, length)\n",
    "\n",
    "    \n",
    "    return(scores_subs, coefs_subs, error_subs) # save last run results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(vector, preprocess = 100000, backup = 'pearson', start=0, end=1, save=True, shuffle=False):\n",
    "    \n",
    "    if vector == 'random_loc':\n",
    "        similarity_location = vector_loc +  backup + '.npy'\n",
    "    else:\n",
    "        similarity_location = vector_loc +  vector + '.npy'\n",
    "    \n",
    "    # loads the similarity values \n",
    "    sim = np.load(similarity_location, allow_pickle=True)\n",
    "    \n",
    "    dis_vector, raw_mastoids, raw_eeg = load_eeg_and_disimilarlity_vector(sim, vector)\n",
    "    data = process_eeg(dis_vector, raw_mastoids, raw_eeg, preprocess)\n",
    "    \n",
    "    scores_subs, coefs_subs, error_subs = TRF(data)\n",
    "    #scores_subs, coefs_subs, error_subs = TRF(data, start=3, end=100, save=save, shuffle=True, second_folder)\n",
    "    \n",
    "    \n",
    "    if end-start > 1:\n",
    "        combine_multi_runs()\n",
    "    \n",
    "    if save:\n",
    "        if vector == 'pearson':\n",
    "            run_string = vector + '_' + str(preprocess)\n",
    "            \n",
    "        elif vector[:4] == 'bert':\n",
    "            run_string = vector + '_' + str(preprocess)\n",
    "            \n",
    "        elif vector == 'static':\n",
    "            run_string = vector + '_' + str(preprocess)\n",
    "        \n",
    "        elif vector == 'random_loc':\n",
    "            run_string = vector + '(' + backup + \")\" + '_' + str(preprocess)\n",
    "        \n",
    "        elif shuffle:\n",
    "            run_string = 'shuffle(' + vector + ')' + '_' + str(preprocess)\n",
    "        \n",
    "        saveFunc(results_folder, run_string, scores_subs, coefs_subs, error_subs, end-start)\n",
    "            \n",
    "    \n",
    "    \n",
    "    return(scores_subs, coefs_subs, error_subs)\n",
    "    \n",
    "#scores_subz, coefs_subz, error_subz = train(vector_loc, method,  999999999999999999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the commands to create the main models used in the thesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores_subz, coefs_subz, error_subz = train('static', 100000) # static_100000.npy\n",
    "# scores_subz, coefs_subz, error_subz = train('pearson', 100000,save=False) # pearson_100000.npy\n",
    "# scores_subz, coefs_subz, error_subz = train('random_loc', 100000, backup = 'pearson') # random_loc_100000.npy\n",
    "\n",
    "#  \n",
    "\n",
    "# scores_subz, coefs_subz, error_subz = train('bert_0_False', 100000) # bert_1_False_100000.npy\n",
    "# scores_subz, coefs_subz, error_subz = train('bert_1_False', 100000) # bert_1_False_100000.npy\n",
    "# scores_subz, coefs_subz, error_subz = train('bert_2_False', 100000) # bert_1_False_100000.npy\n",
    "# scores_subz, coefs_subz, error_subz = train('bert_3_False', 100000) # bert_1_False_100000.npy\n",
    "#scores_subz, coefs_subz, error_subz = train('bert_4_False', 100000) # bert_1_False_100000.npy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# scores_subz, coefs_subz, error_subz = train('bert_0_True', 100000) # bert_1_True_100000.npy\n",
    "# scores_subz, coefs_subz, error_subz = train('bert_1_True', 100000) # bert_1_True_100000.npy\n",
    "# scores_subz, coefs_subz, error_subz = train('bert_2_True', 100000) # bert_1_True_100000.npy\n",
    "# scores_subz, coefs_subz, error_subz = train('bert_3_True', 100000) # bert_1_True_100000.npy\n",
    "# scores_subz, coefs_subz, error_subz = train('bert_4_True', 100000) # bert_1_True_100000.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
